{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lebe1/text-oriented-data-science-project/blob/main/Data_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Dataset"
      ],
      "metadata": {
        "id": "CPLeCMRTwgqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Connect to Google Drive"
      ],
      "metadata": {
        "id": "2p7jTLyRw6g8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO4dyw3uvEz7",
        "outputId": "fc613df0-5bb9-4560-e3f1-536dea69e1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "J051vmiuw_Qv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3LDpTM_YxBaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xqg-zpxvh5s",
        "outputId": "62434df7-cdc1-44ea-e0d0-6c78028a3265"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.20.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import time\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import wandb\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "5r830H34xC1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the CSV File"
      ],
      "metadata": {
        "id": "7VxjImWZyJLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'combined_reviews.csv'\n",
        "file_path = folder_path + file_name\n",
        "\n",
        "#df = pd.read_csv(file_path)\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/DOPP_Ex2_data/reviews_train_long.csv'\n",
        "df = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "3WGFNUyvyFhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "H2PXRsGXQCUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "f4tIP6cjzE2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the model"
      ],
      "metadata": {
        "id": "zL0KD9rDpGxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert only string instances to lowercase\n",
        "    text = text.lower() if isinstance(text, str)  else ''\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize text\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "df['preprocessedText'] = df['reviewText'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize text again for word2vec\n",
        "df['tokenized_text'] = df['preprocessedText'].str.split()\n",
        "\n",
        "df['preprocessedText'].head()\n"
      ],
      "metadata": {
        "id": "F_Kja6GgpIVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['rating']\n",
        "X = pd.DataFrame({'preprocessedText': df['preprocessedText'], 'tokenized_text': df['tokenized_text'], 'reviewText': df[\"reviewText\"]})\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Combine train and test sets\n",
        "df_train = pd.DataFrame({'preprocessedText': X_train['preprocessedText'], 'tokenized_text': X_train['tokenized_text'], 'reviewText': X_train[\"reviewText\"], 'rating': y_train})\n",
        "df_test = pd.DataFrame({'preprocessedText': X_test['preprocessedText'], 'tokenized_text': X_test['tokenized_text'], 'reviewText': X_test[\"reviewText\"], 'rating': y_test})\n",
        "\n",
        "# Save them into csv files\n",
        "df_train.to_csv(folder_path + 'train.csv', index=True)\n",
        "df_test.to_csv(folder_path + 'test.csv', index=True)\n"
      ],
      "metadata": {
        "id": "wHqmS0y7RW39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "noH_t-i_VaEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_texts)"
      ],
      "metadata": {
        "id": "Ijcsp4poG8M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf_train = tfidf.fit_transform(X_train['preprocessedText']).toarray()\n",
        "X_tfidf_test = tfidf.transform(X_test['preprocessedText']).toarray()\n",
        "\n",
        "# Word2Vec Embeddings\n",
        "w2v_model_train = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_sentence_embedding(word_list, model):\n",
        "    word_vecs = [model.wv[word] for word in word_list if word in model.wv]\n",
        "    if word_vecs:\n",
        "        return np.mean(word_vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "X_w2v_train = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in X_train['tokenized_text']])\n",
        "X_w2v_test = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in X_test['tokenized_text']])\n",
        "\n",
        "X_train_vectorized = np.hstack((X_tfidf_train, X_w2v_train))\n",
        "X_test_vectorized = np.hstack((X_tfidf_test, X_w2v_test))\n"
      ],
      "metadata": {
        "id": "oUiXjoEhshIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Including wandb for analysis during model training"
      ],
      "metadata": {
        "id": "C7NbOIFbu0ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "EjDJ7Tltu4rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_project_name = \"DOPP analysis\"\n",
        "wandb_run_name = \"rf_experiment-9-estimators-100\"\n",
        "\n",
        "rf_config = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": None,\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"dataset\": \"Word2Vec\"\n",
        "}\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=wandb_run_name,\n",
        "    config=rf_config\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=rf_config[\"n_estimators\"],\n",
        "    max_depth=rf_config[\"max_depth\"],\n",
        "    random_state=rf_config[\"random_state\"]\n",
        ")\n",
        "rf_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_vectorized)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_rf, average='macro')\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "# Log metrics to W&B\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "n-QpurypvFns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_config = {\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"max_iter\": 1000,\n",
        "    \"penalty\": \"l1\",\n",
        "    \"dataset\": \"Combined\"\n",
        "}\n",
        "\n",
        "wandb_project_name = \"DOPP analysis\"\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=\"svc_experiment-4\",\n",
        "    config=svc_config\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "linear_svc_model = LinearSVC(random_state=svc_config[\"random_state\"], penalty=svc_config[\"penalty\"])\n",
        "linear_svc_model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "y_pred_svc = linear_svc_model.predict(X_test_vectorized)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_svc, average='macro')\n",
        "precision = precision_score(y_test, y_pred_svc, average='macro')\n",
        "recall = recall_score(y_test, y_pred_svc, average='macro')\n",
        "\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "iuDbMxpAyjq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative misclassification analysis"
      ],
      "metadata": {
        "id": "r57xDsUY8kot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_pred_svc))\n",
        "print(np.unique(y_pred_rf))\n"
      ],
      "metadata": {
        "id": "4v7wkbSE8soI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see our models predict all given classes.  \n",
        "Now, let's understand why some classes are misclassified."
      ],
      "metadata": {
        "id": "nXixokC79HHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "false_preds_svc = y_pred_svc != y_test\n",
        "\n",
        "misclassified_predictions = y_pred_svc[false_preds_svc]\n",
        "misclassified_labels = y_test[false_preds_svc]"
      ],
      "metadata": {
        "id": "jwh-vkNS91AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_predictions"
      ],
      "metadata": {
        "id": "m3YPfRJ8B7NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(misclassified_predictions, return_counts=True)"
      ],
      "metadata": {
        "id": "haiVDc4CA0ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By this frequency count, it is observable that most of the time a 5-star-rating is predicting wrong, which makes sense since the original dataset is quite imbalanced. The grade 2 has been misclassified the least but is also the least represented class in the dataset."
      ],
      "metadata": {
        "id": "JAU1pOS4BGv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_labels"
      ],
      "metadata": {
        "id": "nqemDj7o_JUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified = df.iloc[misclassified_labels.index]"
      ],
      "metadata": {
        "id": "u6dHM4So_MEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified[\"misclassified_rating\"] = misclassified_predictions"
      ],
      "metadata": {
        "id": "UeSvwQYs_XHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[0]"
      ],
      "metadata": {
        "id": "IpwFoedk_ttm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[0][\"reviewText\"]"
      ],
      "metadata": {
        "id": "Z-wkKbNk_6g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the review text, we can observe that the model does not really understand the final critizing words of the this review. The review text itself is reasonable to give this three stars."
      ],
      "metadata": {
        "id": "Oa62rV-MARhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[14]"
      ],
      "metadata": {
        "id": "mUY2JVVPAE7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[14][\"reviewText\"]"
      ],
      "metadata": {
        "id": "raM2rqIQAsuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the review text presents some kind of critique, which should be understood by the model not to rate it with five stars."
      ],
      "metadata": {
        "id": "BZqSmedKBifH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[31]"
      ],
      "metadata": {
        "id": "igyy8R42AyAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_misclassified.iloc[31][\"reviewText\"]"
      ],
      "metadata": {
        "id": "mUNUhYMCCDGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an interesting case since we have the opposite behaviour of the model now predicting a higher rated product of 4 with a lower rating of 2.\n",
        "This review text is easy to understand for a human but since we remove stopwords for model training it might be possible that the sentence ends up with complete different meaning with meaningful words like `died` and `carefully`. Based on an assumption like this a two star rating seems plausible."
      ],
      "metadata": {
        "id": "SGGoQH0qCO1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing optimizations"
      ],
      "metadata": {
        "id": "KwJvNsI_DU-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"rating\"].value_counts()"
      ],
      "metadata": {
        "id": "J0KgHybBCM7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have only 306 two star ratings as the least represented class. Therefore, we will take only 306 random samples from the other classes."
      ],
      "metadata": {
        "id": "frJVk71NG_f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating_2 = df_train[df_train[\"rating\"] == 2]"
      ],
      "metadata": {
        "id": "vdKy4WM4G21V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating_1 = df_train[df_train[\"rating\"] == 1].sample(n=306, random_state=42)\n",
        "df_rating_3 = df_train[df_train[\"rating\"] == 3].sample(n=306, random_state=42)\n",
        "df_rating_4 = df_train[df_train[\"rating\"] == 4].sample(n=306, random_state=42)\n",
        "df_rating_5 = df_train[df_train[\"rating\"] == 5].sample(n=306, random_state=42)"
      ],
      "metadata": {
        "id": "xggFIRNLHT0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all dataframes\n",
        "df_balanced = pd.concat([df_rating_2, df_rating_1, df_rating_3, df_rating_4, df_rating_5])"
      ],
      "metadata": {
        "id": "0AMNafikHq-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced.shape"
      ],
      "metadata": {
        "id": "ACL6Sx4OHyA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced[\"rating\"].value_counts()"
      ],
      "metadata": {
        "id": "PlxkOAoeH0g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced.head()"
      ],
      "metadata": {
        "id": "knRAmqLJIAXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_balanced = df_balanced['rating']\n",
        "\n",
        "X_tfidf_train_balanced = tfidf.fit_transform(df_balanced['preprocessedText']).toarray()\n",
        "X_tfidf_test_balanced = tfidf.transform(X_test['preprocessedText']).toarray()\n",
        "\n",
        "# Using w2v model with train set only as recommended here: https://stackoverflow.com/a/70900433/19932351\n",
        "X_w2v_train_balanced = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in df_balanced['tokenized_text']])\n",
        "X_w2v_test_balanced = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in X_test['tokenized_text']])\n",
        "\n",
        "X_train_vectorized_balanced = np.hstack((X_tfidf_train_balanced, X_w2v_train_balanced))\n",
        "X_test_vectorized_balanced = np.hstack((X_tfidf_test_balanced, X_w2v_test_balanced))"
      ],
      "metadata": {
        "id": "5Wmn_ST8IM1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vectorized_balanced.shape"
      ],
      "metadata": {
        "id": "-74S65Fm-7FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_vectorized_balanced.shape"
      ],
      "metadata": {
        "id": "-yRe2HRg-8-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_project_name = \"DOPP analysis\"\n",
        "wandb_run_name = \"rf_balanced-2-estimators-100\"\n",
        "\n",
        "rf_config = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": None,\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"dataset\": \"Balanced-Combined\"\n",
        "}\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=wandb_run_name,\n",
        "    config=rf_config\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=rf_config[\"n_estimators\"],\n",
        "    max_depth=rf_config[\"max_depth\"],\n",
        "    random_state=rf_config[\"random_state\"]\n",
        ")\n",
        "rf_model.fit(X_train_vectorized_balanced, y_train_balanced)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_vectorized_balanced)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_rf, average='macro')\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "# Log metrics to W&B\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9FXZKb2HI8Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_config = {\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"max_iter\": 1000,\n",
        "    \"penalty\": \"l2\",\n",
        "    \"dataset\": \"Balanced-Combined\"\n",
        "}\n",
        "\n",
        "wandb_project_name = \"DOPP analysis\"\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=\"svc_balanced\",\n",
        "    config=svc_config\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "linear_svc_model = LinearSVC(random_state=svc_config[\"random_state\"], penalty=svc_config[\"penalty\"])\n",
        "linear_svc_model.fit(X_train_vectorized_balanced, y_train_balanced)\n",
        "\n",
        "y_pred_svc = linear_svc_model.predict(X_test_vectorized_balanced)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_svc, average='macro')\n",
        "precision = precision_score(y_test, y_pred_svc, average='macro')\n",
        "recall = recall_score(y_test, y_pred_svc, average='macro')\n",
        "\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "QgkPQQrsJKxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the metrices, we can observe that the random forest improves in terms of predicting several classes, whereas the svc model trained with a balanced dataset results in a lower performance not recognizing the majority class that well anymore."
      ],
      "metadata": {
        "id": "MVa1m6jpKuEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Oversampling\n"
      ],
      "metadata": {
        "id": "ulrAwvxgLEI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating_1 = df_train[df_train[\"rating\"] == 1]\n",
        "df_rating_2 = df_train[df_train[\"rating\"] == 2]\n",
        "df_rating_3 = df_train[df_train[\"rating\"] == 3]\n",
        "df_rating_4 = df_train[df_train[\"rating\"] == 4]\n",
        "df_rating_5 = df_train[df_train[\"rating\"] == 5]\n",
        "print(\"1\",len(df_rating_1))\n",
        "print(\"2\", len(df_rating_2))\n",
        "print(\"3\", len(df_rating_3))\n",
        "print(\"4\",len(df_rating_4))\n",
        "print(\"5\", len(df_rating_5))"
      ],
      "metadata": {
        "id": "Z7pFqKqqLFeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our next approach, we target the sample count of 1414, which is the count of class 4, the second most represented class. Therefore, we have to oversample classes 1 to 3 and undersample the majority class 5."
      ],
      "metadata": {
        "id": "tFm2MKTM2Zx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating_1 = df_train[df_train[\"rating\"] == 1].sample(n=1414, random_state=42, replace=True)\n",
        "df_rating_2 = df_train[df_train[\"rating\"] == 2].sample(n=1414, random_state=42, replace=True)\n",
        "df_rating_3 = df_train[df_train[\"rating\"] == 3].sample(n=1414, random_state=42, replace=True)\n",
        "df_rating_5 = df_train[df_train[\"rating\"] == 5].sample(n=1414, random_state=42)"
      ],
      "metadata": {
        "id": "3m7DA_WF18UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oversampled = pd.concat([df_rating_2, df_rating_1, df_rating_3, df_rating_4, df_rating_5])"
      ],
      "metadata": {
        "id": "zK23vWNjMcnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_oversampled[\"rating\"].value_counts()"
      ],
      "metadata": {
        "id": "eMjNrxfLMnta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_oversampled = df_oversampled['rating']\n",
        "\n",
        "X_tfidf_train_oversampled = tfidf.fit_transform(df_oversampled['preprocessedText']).toarray()\n",
        "X_tfidf_test_oversampled = tfidf.transform(X_test['preprocessedText']).toarray()\n",
        "\n",
        "# Using w2v model with train set only as recommended here: https://stackoverflow.com/a/70900433/19932351\n",
        "X_w2v_train_oversampled = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in df_oversampled['tokenized_text']])\n",
        "X_w2v_test_oversampled = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in X_test['tokenized_text']])\n",
        "\n",
        "X_train_vectorized_oversampled = np.hstack((X_tfidf_train_oversampled, X_w2v_train_oversampled))\n",
        "X_test_vectorized_oversampled = np.hstack((X_tfidf_test_oversampled, X_w2v_test_oversampled))\n"
      ],
      "metadata": {
        "id": "m4frNPXfgGgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_project_name = \"DOPP analysis\"\n",
        "wandb_run_name = \"rf_oversampled\"\n",
        "\n",
        "rf_config = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": None,\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"dataset\": \"Oversampled-Combined\"\n",
        "}\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=wandb_run_name,\n",
        "    config=rf_config\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=rf_config[\"n_estimators\"],\n",
        "    max_depth=rf_config[\"max_depth\"],\n",
        "    random_state=rf_config[\"random_state\"]\n",
        ")\n",
        "rf_model.fit(X_train_vectorized_oversampled, y_train_oversampled)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_vectorized_oversampled)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_rf, average='macro')\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "# Log metrics to W&B\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "OP2oVuroM0J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_config = {\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"max_iter\": 1000,\n",
        "    \"penalty\": \"l2\",\n",
        "    \"dataset\": \"Oversampled-Combined\"\n",
        "}\n",
        "\n",
        "wandb_project_name = \"DOPP analysis\"\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=\"svc_balanced\",\n",
        "    config=svc_config\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "linear_svc_model = LinearSVC(random_state=svc_config[\"random_state\"], penalty=svc_config[\"penalty\"])\n",
        "linear_svc_model.fit(X_train_vectorized_oversampled, y_train_oversampled)\n",
        "\n",
        "y_pred_svc = linear_svc_model.predict(X_test_vectorized_oversampled)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_svc, average='macro')\n",
        "precision = precision_score(y_test, y_pred_svc, average='macro')\n",
        "recall = recall_score(y_test, y_pred_svc, average='macro')\n",
        "\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "tVKMzS6kNED7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "N5GpjAk5UmtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/DOPP_Ex2_data/reviews_train_long.csv'\n",
        "df_da = pd.read_csv(csv_path)"
      ],
      "metadata": {
        "id": "zcagKg_vUu19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_da = df_da.sample(frac=1).reset_index(drop=True)\n",
        "df_da.head(2)"
      ],
      "metadata": {
        "id": "A00RUrAvVIlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_da['preprocessedText'] = df_da['reviewText'].apply(preprocess_text)\n",
        "df_da['tokenized_text'] = df_da['preprocessedText'].str.split()"
      ],
      "metadata": {
        "id": "6b3LM6deVg6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_da = df_da['rating']\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf_train_da =  tfidf.fit_transform(df_da['preprocessedText']).toarray()\n",
        "X_tfidf_test_da = tfidf.transform(X_test['preprocessedText']).toarray()\n",
        "\n",
        "# Using w2v model with train set only as recommended here: https://stackoverflow.com/a/70900433/19932351\n",
        "X_w2v_train_da = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in df_da['tokenized_text']])\n",
        "X_w2v_test_da = np.array([get_sentence_embedding(word_list, w2v_model_train) for word_list in X_test['tokenized_text']])\n",
        "\n",
        "X_train_vectorized_da = np.hstack((X_tfidf_train_da, X_w2v_train_da))\n",
        "X_test_vectorized_da = np.hstack((X_tfidf_test_da, X_w2v_test_da))"
      ],
      "metadata": {
        "id": "FRRE4P59NXtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_project_name = \"DOPP analysis\"\n",
        "wandb_run_name = \"rf_da\"\n",
        "\n",
        "rf_config = {\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": None,\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"dataset\": \"Data-Augmentation\"\n",
        "}\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=wandb_run_name,\n",
        "    config=rf_config\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=rf_config[\"n_estimators\"],\n",
        "    max_depth=rf_config[\"max_depth\"],\n",
        "    random_state=rf_config[\"random_state\"]\n",
        ")\n",
        "rf_model.fit(X_train_vectorized_da, y_train_da)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test_vectorized_da)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_rf, average='macro')\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "# Log metrics to W&B\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "40EczCOxV0Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_config = {\n",
        "    \"random_state\": 42,\n",
        "    \"test_size\": 0.2,\n",
        "    \"max_iter\": 1000,\n",
        "    \"penalty\": \"l2\",\n",
        "    \"dataset\": \"Data-Augmentation\"\n",
        "}\n",
        "\n",
        "wandb_project_name = \"DOPP analysis\"\n",
        "\n",
        "wandb.init(\n",
        "    project=wandb_project_name,\n",
        "    name=\"svc_balanced\",\n",
        "    config=svc_config\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "linear_svc_model = LinearSVC(random_state=svc_config[\"random_state\"], penalty=svc_config[\"penalty\"])\n",
        "linear_svc_model.fit(X_train_vectorized_da, y_train_da)\n",
        "\n",
        "y_pred_svc = linear_svc_model.predict(X_test_vectorized_da)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "f1Score = f1_score(y_test, y_pred_svc, average='macro')\n",
        "precision = precision_score(y_test, y_pred_svc, average='macro')\n",
        "recall = recall_score(y_test, y_pred_svc, average='macro')\n",
        "\n",
        "wandb.log({\n",
        "    \"Execution Time\": execution_time,\n",
        "    \"F1 Score\": f1Score,\n",
        "    \"Precision Score\": precision,\n",
        "    \"Recall Score\": recall\n",
        "})\n",
        "\n",
        "print(\"Execution Time:\", execution_time, \"seconds\")\n",
        "print(\"Precision Score:\", precision)\n",
        "print(\"Recall Score:\", recall)\n",
        "print(\"F1 Score:\", f1Score, \"\\n\")\n",
        "print(classification_report(y_test, y_pred_svc))\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "NGMgTQMrWPeP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}